{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d0a5e4f-9e07-4b0e-956c-bb2ed2bfa034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5080a63b-c89b-492c-9cff-de822374f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define general functions.\n",
    "\n",
    "def find_paths(variable, df, path):\n",
    "    # Add the current path of the variable\n",
    "    path.append(variable)\n",
    "    \n",
    "    # Find all variables that depend on the current variable\n",
    "    next_steps = df[df['Dependency'] == variable]\n",
    "    \n",
    "    # If no further dependencies are found, return the current path\n",
    "    if next_steps.empty:\n",
    "        return [path]\n",
    "    \n",
    "    # List to store all paths\n",
    "    all_paths = []\n",
    "    \n",
    "    # For each next variable, recursively continue following the path\n",
    "    for _, row in next_steps.iterrows():\n",
    "        new_paths = find_paths(row['Variable'], df, path.copy())  # Copy the path to allow branching\n",
    "        all_paths.extend(new_paths)\n",
    "    \n",
    "    return all_paths\n",
    "\n",
    "def extract_code_from_notebook(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        notebook = json.load(f)\n",
    "    \n",
    "    # Combine all code lines from each cell\n",
    "    code_cells = [\"\".join(cell['source']) for cell in notebook['cells'] if cell['cell_type'] == 'code']\n",
    "    return \"\\n\".join(code_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7125d8f7-0d8f-4f37-a0f9-079575222063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Test Functions\n",
    "def check_row_count(df):\n",
    "    # Überprüft, ob der DataFrame mehr als 0 Zeilen hat\n",
    "    return 1 if df.shape[0] > 0 else 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8fad87db-2be1-4be6-92db-e3661988424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data 1\n",
    "df1_read = pd.read_csv(r\"C:\\Users\\Robin\\OneDrive\\Blog\\Detect Workflow Errows\\order_table_v2.csv\")\n",
    "\n",
    "# Load Data 2\n",
    "df2_read = pd.read_csv(r\"C:\\Users\\Robin\\OneDrive\\Blog\\Detect Workflow Errows\\customer_table.csv\")\n",
    "\n",
    "# 1.2 Adjust column\n",
    "df1_change_datatype_orderdate = df1_read.assign(order_date=pd.to_datetime(df1_read['order_date']))\n",
    "\n",
    "#1.3 Add new column\n",
    "df1_add_column_year = df1_change_datatype_orderdate.assign(year=lambda x: x['order_date'].dt.year)\n",
    "\n",
    "#1.4 Filter year 2023\n",
    "df1_filtered_year = df1_add_column_year[df1_add_column_year['year']==2023]\n",
    "\n",
    "#1.5 Aggregate\n",
    "df1_aggregated = df1_filtered_year.groupby(['customer_id']).agg(\n",
    "    total_price=('total_price', 'sum'),\n",
    "    unique_order=('order_id', 'nunique')\n",
    ").reset_index()\n",
    "\n",
    "# 1.6 merge\n",
    "merged_df1_df2 = pd.merge(df1_aggregated,df2_read,left_on='customer_id',right_on='id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2780edf4-984e-4d29-bdb7-bba8ecfd50c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires a specific structure of the DataFrames. \n",
    "# 1. When making transformations on existing DataFrames, they must be saved in new DataFrames.\n",
    "# 2. All DataFrames must have a unique name and start with 'df' followed by the original number.\n",
    "\n",
    "file_path = r\"C:\\Users\\Robin\\OneDrive\\Blog\\Detect Workflow Errows\\01_simple_testing_framework.ipynb\"\n",
    "json_text = extract_code_from_notebook(file_path)\n",
    "json_splitted = json_text.split('\\n')\n",
    "\n",
    "# Regular expression pattern to filter 'df' followed by a number\n",
    "pattern = r'(df\\d+)'  # 'df' followed by one or more digits\n",
    "\n",
    "# Filtering the variables\n",
    "filtered_variables = [line for line in json_splitted if re.search(pattern, line)]\n",
    "variables_dataframe = pd.DataFrame(filtered_variables)\n",
    "variables_dataframe= variables_dataframe[variables_dataframe[0].str.contains('=')]\n",
    "\n",
    "iteration = 0\n",
    "variables_dataframe['variable'] = \"\"\n",
    "variables_dataframe['function'] = \"\"\n",
    "for i, r in variables_dataframe.iterrows():\n",
    "    variables_dataframe['variable'][iteration] = r[0].split('=', 1)[0]\n",
    "    variables_dataframe['function'][iteration] = r[0].split('=', 1)[1]\n",
    "    iteration += 1\n",
    "\n",
    "variable_without_duplicates = variables_dataframe[['variable']].drop_duplicates()\n",
    "\n",
    "variable = []\n",
    "dependency = []\n",
    "\n",
    "for i, r in variables_dataframe.iterrows():\n",
    "    for j, s in variable_without_duplicates.iterrows():\n",
    "        if s['variable'].strip() in r['function'].strip():\n",
    "            dependency += [s['variable'].strip()]\n",
    "        else:\n",
    "            dependency += [None]\n",
    "        variable += [r['variable'].strip()]\n",
    "        \n",
    "relationship = pd.DataFrame({\n",
    "    'Variable': variable,\n",
    "    'Dependency': dependency\n",
    "})\n",
    "\n",
    "relationship_filtered = relationship[\n",
    "    relationship['Dependency'].notnull() | \n",
    "    relationship['Variable'].str.contains('read')\n",
    "]\n",
    "relationship_dropduplicates = relationship_filtered.drop_duplicates()\n",
    "relationship_dropduplicates = relationship_dropduplicates[relationship_dropduplicates['Variable'] != 'tested_dataframes']\n",
    "\n",
    "relationship_dropduplicates_readonly = relationship_dropduplicates[relationship_dropduplicates['Dependency'].isnull()]\n",
    "\n",
    "stage = []\n",
    "variable_list = []\n",
    "\n",
    "for i, r in relationship_dropduplicates_readonly.iterrows():\n",
    "    start = relationship_dropduplicates[relationship_dropduplicates['Variable'] == r['Variable']]\n",
    "    stage_num = 0\n",
    "    while True:\n",
    "        stage_num += 1\n",
    "        stage += [stage_num] * len(start)  # Add the current stage number for all rows\n",
    "        variable_list += start['Variable'].tolist()  # Add all variable values as a list\n",
    "        start = start[['Variable']]\n",
    "        start = start.rename(columns={'Variable': 'Dependency'})\n",
    "        start = pd.merge(relationship_dropduplicates, start, on='Dependency')\n",
    "        if len(start) == 0:\n",
    "            break\n",
    "\n",
    "result_stages = pd.DataFrame({'Variable': variable_list, 'Stages': stage})\n",
    "result_stages = result_stages.groupby('Variable').agg({'Stages': 'max'}).reset_index()\n",
    "\n",
    "# Create Coordinates\n",
    "\n",
    "# List to store all paths\n",
    "result_paths = []\n",
    "\n",
    "# Recursively traverse the path for each starting point\n",
    "for start in relationship_dropduplicates_readonly[relationship_dropduplicates_readonly['Dependency'].isnull()]['Variable']:\n",
    "    paths = find_paths(start, relationship_dropduplicates, [])\n",
    "    result_paths.extend(paths)\n",
    "branch = []\n",
    "Variable = []\n",
    "\n",
    "# Output results\n",
    "for i, path in enumerate(result_paths, 1):\n",
    "    Variable += [path]\n",
    "    branch += [i]\n",
    "\n",
    "result_branch = pd.DataFrame({'Variable': Variable, 'Branch': branch})\n",
    "result_branch = result_branch.explode('Variable').reset_index(drop=True)\n",
    "result_branch = result_branch.groupby('Variable').agg({'Branch': 'min'}).reset_index()\n",
    "\n",
    "result_branch_stage = pd.merge(result_branch, result_stages, on='Variable')\n",
    "\n",
    "# Line Construction\n",
    "relationship_dropduplicates_excluderead = relationship_dropduplicates[relationship_dropduplicates['Dependency'].notnull()]\n",
    "relationship_dropduplicates_excluderead = relationship_dropduplicates_excluderead.reset_index(drop=True)\n",
    "relationship_dropduplicates_excluderead['RowNumber'] = relationship_dropduplicates_excluderead.index + 1\n",
    "\n",
    "# Transform the DataFrame to bring the other columns into rows\n",
    "relationship_melted = relationship_dropduplicates_excluderead.melt(id_vars=['RowNumber'], var_name='Variable', value_name='Value')\n",
    "relationship_melted = relationship_melted[['RowNumber', 'Value']]\n",
    "relationship_merged = pd.merge(result_branch_stage, relationship_melted, left_on='Variable', right_on='Value')\n",
    "\n",
    "relationship_merged = relationship_merged[['Variable', 'Branch', 'Stages', 'RowNumber']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "873a3dab-2bb8-4c86-abe0-2508de38e59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_variable = []  # List to store the names of the tested variables\n",
    "test_result = []    # List to store the results of the tests\n",
    "\n",
    "# List of DataFrames to be tested\n",
    "tested_dataframes = [\n",
    "    'df1_read', \n",
    "    'df1_change_datatype_orderdate', \n",
    "    'df1_add_column_year', \n",
    "    'df1_add_column_year', \n",
    "    'df1_filtered_year', \n",
    "    'df1_aggregated', \n",
    "    'merged_df1_df2', \n",
    "    'df2_read'\n",
    "]\n",
    "\n",
    "# Iterate over each DataFrame name in the tested_dataframes list\n",
    "for i in tested_dataframes:\n",
    "    # Append the result of the row count check for the current DataFrame to the test_result list\n",
    "    test_result += [check_row_count(eval(i))]\n",
    "    # Append the current DataFrame name to the test_variable list\n",
    "    test_variable += [i]\n",
    "\n",
    "# Create a DataFrame to store the test results with variable names and their corresponding results\n",
    "result_test_dataframe = pd.DataFrame({'Variable': test_variable, 'Test_Result': test_result})\n",
    "\n",
    "# Summarize the test results by taking the minimum result for each variable\n",
    "result_test_dataframe_summarized = result_test_dataframe.groupby('Variable').agg({'Test_Result': 'min'}).reset_index()\n",
    "\n",
    "# Merge the summarized test results with the relationship_merged DataFrame\n",
    "endresult = pd.merge(relationship_merged, result_test_dataframe_summarized, on='Variable', how='left')\n",
    "\n",
    "endresult['Test_Result'] = endresult['Test_Result'].fillna(0)\n",
    "\n",
    "\n",
    "# Save the final result to an Excel file\n",
    "endresult.to_excel(r\"C:\\Users\\Robin\\OneDrive\\Blog\\Detect Workflow Errows\\line_coordninates.xlsx\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
